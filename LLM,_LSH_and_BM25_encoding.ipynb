{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Packages Installation, Library Imports, and Data Imports\n"
      ],
      "metadata": {
        "id": "-HsLXSFxsne0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bi1OCv1xDjg"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1SS7xAwOG-1i"
      },
      "outputs": [],
      "source": [
        "%pip install faiss-gpu\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "!pip install sentence-transformers\n",
        "!pip install rank_bm25\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install torch\n",
        "!pip install SentencePiece\n",
        "!pip install evaluate\n",
        "!pip install bert_score\n",
        "!pip install chromadb\n",
        "!pip install transformers\n",
        "!pip install ctransformers ctransformers[cuda]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "edzDSBdHCS7N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "import accelerate\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import numpy as np\n",
        "import chromadb\n",
        "import random\n",
        "import evaluate\n",
        "import math\n",
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from chromadb.utils import embedding_functions\n",
        "from transformers import LlamaTokenizer,GenerationConfig\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZHOaBGr3CLmq"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXy0ubCEKMzr"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ejYGXC-vLW0U"
      },
      "outputs": [],
      "source": [
        "class Document:\n",
        "  def __init__(self, row):\n",
        "    self.details = row\n",
        "\n",
        "  @property\n",
        "  def info(self):\n",
        "    return self.details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XSE2-KZrxg8T"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(sentence):\n",
        "    tokens = [word for word in sentence.split() if word.lower() not in set(stopwords.words('english'))]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KC4OTVbNEff"
      },
      "outputs": [],
      "source": [
        "class Job(Document):\n",
        "  def __init__(self, row):\n",
        "    super().__init__(row)\n",
        "    self.open_job_href = row[\"open_job-href\"]\n",
        "    self.job_title = row[\"job_title_simple\"]\n",
        "    self.company_name = row[\"company_name\"]\n",
        "    self.address = row[\"address\"]\n",
        "    self.salary_lb = row[\"salary_lb\"]\n",
        "    self.salary_hb = row[\"salary_hb\"]\n",
        "    self. job_desc = row[\"job_desc\"]\n",
        "\n",
        "  @property\n",
        "  def job_text(self):\n",
        "    return (\n",
        "        f\"\"\"Job title: {self.job_title}\n",
        "        Job link: {self.open_job_href}\n",
        "        Company name: {self.company_name}\n",
        "        Address: {self.address}\n",
        "        Salary: {self.salary_lb} - {self.salary_hb}\n",
        "        Job Description: {self.job_desc}\n",
        "        \"\"\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWzU-gpX7Wbl"
      },
      "outputs": [],
      "source": [
        "class Course(Document):\n",
        "    def __init__(self, row):\n",
        "      super().__init__(row)\n",
        "      self.url = row[\"url\"]\n",
        "      self.course_desc = row[\"description\"]\n",
        "\n",
        "    @property\n",
        "    def course_text(self):\n",
        "      return (\n",
        "          f\"\"\"Course link: {self.url}\n",
        "          Course Description: {self.course_desc}\n",
        "          \"\"\"\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu4xorT4Lm-U"
      },
      "source": [
        "# LLM Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-6ywkj-n-n2"
      },
      "outputs": [],
      "source": [
        "def load_vicuna():\n",
        "    from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "    load_in_8bit = True\n",
        "    model_name = \"lmsys/vicuna-7b-v1.5\"\n",
        "\n",
        "    model_v = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_8bit=load_in_8bit,\n",
        "        device_map=\"auto\"\n",
        "        )\n",
        "    return model_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPF1EbCYL-N0"
      },
      "outputs": [],
      "source": [
        "def response_gen(input_ids):\n",
        "  temperature=0.7\n",
        "  with torch.no_grad():\n",
        "      generation_output = model_l.generate(\n",
        "          input_ids=input_ids,\n",
        "          temperature=temperature,\n",
        "          top_p = 1.0,\n",
        "          do_sample=True,\n",
        "          return_dict_in_generate=True,\n",
        "          max_new_tokens=300,\n",
        "      )\n",
        "  s = generation_output.sequences[0][len(input_ids[0]):]\n",
        "  output = tokenizer_l.decode(s)\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j81gleXwmgcF"
      },
      "outputs": [],
      "source": [
        "def load_openhermes():\n",
        "  from ctransformers import AutoModelForCausalLM\n",
        "  model_name_l = \"TheBloke/OpenHermes-2.5-neural-chat-7B-v3-1-7B-GGUF\"\n",
        "\n",
        "  model_oh = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_l,\n",
        "    model_file=\"openhermes-2.5-neural-chat-7b-v3-1-7b.Q4_K_M.gguf\",\n",
        "    model_type=\"mistral\",\n",
        "    gpu_layers=100,\n",
        "    max_new_tokens = 300,\n",
        "    context_length = 2048\n",
        "    )\n",
        "  return model_oh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Bp5n3hFNHG"
      },
      "source": [
        "# Search Indexing and Similarity Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGYkk-Z70eZs"
      },
      "outputs": [],
      "source": [
        "class IRModel:\n",
        "\n",
        "  def init_model(self, mode):\n",
        "    if mode == 'BERT': # vector dim --> (n, 768)\n",
        "      model_name='bert-base-nli-mean-tokens'\n",
        "      model = SentenceTransformer(model_name)\n",
        "    if mode == 'MINILM': # vector dim --> (n, 384)\n",
        "      model_name= 'all-MiniLM-L6-v2'\n",
        "      model = SentenceTransformer(model_name)\n",
        "    elif mode == 'D2V': # vector dim --> (n, 768)\n",
        "      model = Doc2Vec(vector_size=768, min_count=2, epochs=50)\n",
        "\n",
        "    self.mode = mode\n",
        "    return model\n",
        "\n",
        "  def doc_to_vector(self, model, arr):\n",
        "      if self.mode == 'BERT' or self.mode == 'MINILM':\n",
        "        return self.transformer_vector(model, arr)\n",
        "      if self.mode == 'D2V':\n",
        "        return self.doc2vec_vector(model, arr)\n",
        "\n",
        "  def doc2vec_vector(self, model, arr):\n",
        "    tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(arr)]\n",
        "\n",
        "    model.build_vocab(tagged_data)\n",
        "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "    # get the document vectors\n",
        "    vectors = np.array([model.infer_vector(word_tokenize(doc.lower())) for doc in arr])\n",
        "    vector_doc_map = {tuple(vec): doc for vec, doc in zip(vectors, arr)}\n",
        "\n",
        "    return vectors, vector_doc_map\n",
        "\n",
        "  def transformer_vector(self, model, arr):\n",
        "    # create vector embeddings for the documents\n",
        "    vectors = [model.encode(s) for s in arr]\n",
        "    vectors = np.array(vectors, dtype=np.float32)\n",
        "    # Create a dictionary mapping vector to document\n",
        "    vector_doc_map = {tuple(vec): doc for vec, doc in zip(vectors, arr)}\n",
        "    return vectors, vector_doc_map\n",
        "\n",
        "  def word_to_vector(self, model, word):\n",
        "    if self.mode == 'BERT' or self.mode == 'MINILM':\n",
        "      return model.encode(word)\n",
        "    elif self.mode == 'D2V':\n",
        "      return model.infer_vector(word_tokenize(word.lower()))\n",
        "\n",
        "  def lsh_index_search(self, wb, xq, vector_doc_map, k):\n",
        "    d = wb.shape[1]\n",
        "    nbits = 1000\n",
        "    # initialize the index using our doc_wb dimensionality (128) and nbits\n",
        "    index = faiss.IndexLSH(d, nbits)\n",
        "    # then add the data\n",
        "    index.add(wb)\n",
        "    xq0 = xq.reshape(1, -1)\n",
        "\n",
        "    D, I = index.search(xq0, k=k)\n",
        "\n",
        "    rel_doc = self.get_relevant_doc(wb[I[0]], vector_doc_map)\n",
        "    return wb[I[0]], rel_doc\n",
        "\n",
        "  def bm25_search(self, arr, query_string, vector_doc_map, k):\n",
        "    # initialize bm25 object and add in documents\n",
        "    tokenized_corpus = [doc.split(\" \") for doc in arr]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "    # create vector embedding for query\n",
        "    tokenized_query = query_string.split(\" \")\n",
        "    doc_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # get top n relevant jobs\n",
        "    relevant_docs = bm25.get_top_n(tokenized_query, arr, n=k)\n",
        "\n",
        "    # get vector of top n relevant jobs\n",
        "    relevant_docs_vectors = []\n",
        "    for doc in relevant_docs:\n",
        "      for key, val in vector_doc_map.items():\n",
        "        if val == doc:\n",
        "          relevant_docs_vectors.append(key)\n",
        "\n",
        "    return relevant_docs_vectors, relevant_docs\n",
        "\n",
        "  def setup_chromadb(self,arr,vectors,course_or_job):\n",
        "    CHROMA_DATA_PATH = \"chroma_data/\"\n",
        "    EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "    COLLECTION_NAME = course_or_job + str(random.randrange(1000))\n",
        "\n",
        "    client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
        "    # embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBED_MODEL)\n",
        "\n",
        "    collection = client.create_collection(\n",
        "        name=COLLECTION_NAME,\n",
        "        # embedding_function=embedding_func,\n",
        "        metadata={\"hnsw:space\": \"cosine\"},\n",
        "    )\n",
        "    collection.add(embeddings=vectors,documents=arr,ids=[f\"id{i}\" for i in range(len(arr))])\n",
        "    return collection\n",
        "\n",
        "  def chromadb_search(self, collection, query_vector, vector_doc_map, k):\n",
        "    collection = collection\n",
        "    query_results = collection.query(query_embeddings=query_vector,n_results=k)\n",
        "    relevant_docs = query_results[\"documents\"][0]\n",
        "    # get vector of top n relevant jobs\n",
        "    relevant_docs_vectors = []\n",
        "    for doc in relevant_docs:\n",
        "      for key, val in vector_doc_map.items():\n",
        "        if val == doc:\n",
        "          relevant_docs_vectors.append(key)\n",
        "\n",
        "    return relevant_docs_vectors, relevant_docs\n",
        "\n",
        "  def get_relevant_doc(self, vector_docs, vector_doc_map):\n",
        "    relevant_documents = []\n",
        "    # Retrieve the original document based on a vector\n",
        "    for i in range(len(vector_docs)):\n",
        "        retrieved_document = vector_doc_map.get(tuple(vector_docs[i]), \"Document not found\")\n",
        "        relevant_documents.append(retrieved_document)\n",
        "    return relevant_documents\n",
        "\n",
        "  def cos_sim(self, relevant_doc_vector, xq):\n",
        "    if len(relevant_doc_vector) == 0 :\n",
        "      return \"No document to compare for cosine similarity\"\n",
        "    return cosine_similarity(relevant_doc_vector, [xq])\n",
        "\n",
        "  def dot_produdct_sim(self, relevant_doc_vector, xq):\n",
        "    if len(relevant_doc_vector) == 0 :\n",
        "      raise Exception(\"No document to compare\")\n",
        "    vector1 = np.array(relevant_doc_vector)\n",
        "    vector2 = np.array(xq)\n",
        "    dot_product = np.dot(vector1, vector2)\n",
        "    return dot_product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugn27N04fC6G"
      },
      "outputs": [],
      "source": [
        "def getAnswer(prompt, rel_doc, llm_mode):\n",
        "    if llm_mode == 'VICUNA':\n",
        "      output = response_gen(prompt)\n",
        "    elif llm_mode == 'OPENHERMES':\n",
        "      output = model_l(prompt, stream=False)\n",
        "\n",
        "    pred_vector = irModel.word_to_vector(model,output)\n",
        "    # query_vector = irModel.word_to_vector(model, question)\n",
        "    similarity = irModel.cos_sim(pred_vector.reshape(1,-1), rel_doc)\n",
        "\n",
        "    return output, similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzHZE3tkDVvt"
      },
      "source": [
        "# Heuristics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRDXnI6rDZvN"
      },
      "outputs": [],
      "source": [
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, dim_model=50, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden = nn.Linear(dim_model, dim_model)\n",
        "        self.classification_head = nn.Linear(dim_model, 1)\n",
        "\n",
        "    def forward(self, src: torch.tensor) -> torch.tensor:\n",
        "        for i in range(self.num_layers):\n",
        "          src = self.hidden(src)\n",
        "          src = torch.sigmoid(src)\n",
        "\n",
        "        src = self.classification_head(src)\n",
        "        src = torch.sigmoid(src)\n",
        "        src = src.squeeze(-1)\n",
        "\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWNLe4zwDeMt"
      },
      "outputs": [],
      "source": [
        "class Heuristic():\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    checkpoint_path = 'drive/MyDrive/IR_Project/heuristic_checkpoint.pt'\n",
        "    if os.path.exists(checkpoint_path):\n",
        "      self.model = LinearModel()\n",
        "      checkpoint = torch.load(checkpoint_path)\n",
        "      self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      self.model = self.model.to(\"cuda\")\n",
        "    else:\n",
        "      raise ValueError(\"Cannot find checkpoint path\")\n",
        "\n",
        "  def process(self, label_features, weight=0.5):\n",
        "    the_feat = self.__process_tokenized(label_features)\n",
        "    theProb = self.model(the_feat).item()\n",
        "    outputprob = weight*theProb + 0.5*(1-weight)\n",
        "    return outputprob\n",
        "\n",
        "  def __process_tokenized(self, label_features, max_tokens=50):\n",
        "    label_feature = label_features[\"input_ids\"].to(\"cuda\")\n",
        "    num_frames = label_feature.shape[1]\n",
        "    if num_frames < max_tokens:\n",
        "        padding = torch.zeros(label_feature.shape[0], max_tokens - num_frames).to(\"cuda\")\n",
        "        label_feature = torch.cat((label_feature, padding), dim=1)\n",
        "    elif num_frames > max_tokens:\n",
        "        label_feature = label_feature[:, :max_tokens]\n",
        "    return label_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGQ3b8ZyJUiV"
      },
      "source": [
        "# Self Consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PypXKYXZMgyT"
      },
      "outputs": [],
      "source": [
        "def getPredictionSC(context, question,n):\n",
        "  # print(\"Questions:\", question)\n",
        "  ignored_answer = '</s>'\n",
        "  predicted_answers = {}\n",
        "  for i in range(n):\n",
        "    prompt = f\"Please answer the question based on the context.{context}\\n Question: {question}\"\n",
        "\n",
        "    if llm_mode == 'VICUNA':\n",
        "      input = tokenizer_l(prompt, return_tensors='pt')\n",
        "      input_ids = input[\"input_ids\"].to(\"cuda\")\n",
        "      output = response_gen(input_ids)\n",
        "    elif llm_mode == 'OPENHERMES':\n",
        "      output = model_l(prompt, stream=False)\n",
        "\n",
        "    print(f\"Predicted answer at i: {i} ==> {output}\")\n",
        "\n",
        "    if n <= 1:\n",
        "      return output\n",
        "\n",
        "    if output != ignored_answer:\n",
        "      if output not in predicted_answers.keys() :\n",
        "        predicted_answers[output] = 1\n",
        "      predicted_answers[output] +=1\n",
        "\n",
        "  if len(predicted_answers.keys()) != 0:\n",
        "    return max(predicted_answers, key=predicted_answers.get)\n",
        "  else:\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterative Refinement"
      ],
      "metadata": {
        "id": "3sH6--9StcmG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HbJlBgTIUuJ"
      },
      "outputs": [],
      "source": [
        "def getPredictionLOOOP(context, question,rel_doc,n, llm_mode):\n",
        "  ignored_answer = '</s>'\n",
        "  predicted_answers = {}\n",
        "  prompt = f\"Please answer the question based on the context.{context}\\n Question: {question}\"\n",
        "  if llm_mode == 'VICUNA':\n",
        "      prompt = tokenizer_l(prompt, return_tensors='pt')\n",
        "      prompt = prompt[\"input_ids\"].to(\"cuda\")\n",
        "\n",
        "  for i in range(n):\n",
        "    # print(\"----------- Questions with EP:\", prompt)\n",
        "    output, similarity = getAnswer(prompt, rel_doc, llm_mode)\n",
        "\n",
        "    print(f\"Predicted Looop answer ==> {output}\")\n",
        "    print(f\"Similarity Looop ==> {similarity}\")\n",
        "\n",
        "    if output != ignored_answer:\n",
        "      predicted_answers[output] = similarity\n",
        "      if n <= 1:\n",
        "        return output, similarity\n",
        "\n",
        "  if len(predicted_answers.keys()) != 0:\n",
        "    print(\"////////// Predicted Looop Answers : \", predicted_answers)\n",
        "    theKey = max(predicted_answers, key=predicted_answers.get)\n",
        "    return theKey, predicted_answers[theKey]\n",
        "  else:\n",
        "    return \"\",0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1MFJH0TUkuj"
      },
      "source": [
        "# Emotional Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMNXKC2XUGsU"
      },
      "outputs": [],
      "source": [
        "def getPredictionEP(context, question,rel_doc, n,llm_mode):\n",
        "  EPS1 = [\"This is very important to me.\",\n",
        "          \"You'd better be sure.\",\n",
        "          \"Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.\",\n",
        "          \"Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.\",\n",
        "          \"Take pride in your work and give it your best. Your commitment to excellence sets you apart.\",\n",
        "          \"Remember that progress is made one step at a time. Stay determined and keep moving forward.\"]\n",
        "\n",
        "  EPS2 = [\n",
        "      \"Are you sure?\",\n",
        "      \"Are you sure that is your final answer? It might be worth taking another look.\",\n",
        "      \"Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.\",\n",
        "  ]\n",
        "\n",
        "  ignored_answer = '</s>'\n",
        "  predicted_answers = {}\n",
        "\n",
        "  prompt = f\"Please answer the question based on the context.{context}\\n Question: {question}\\n {EPS2[0]}\"\n",
        "  if llm_mode == 'VICUNA':\n",
        "      prompt = tokenizer_l(prompt, return_tensors='pt')\n",
        "      prompt = input[\"input_ids\"].to(\"cuda\")\n",
        "\n",
        "  for EP in EPS1[:1]:\n",
        "    output, similarity = getAnswer(prompt, rel_doc,llm_mode)\n",
        "\n",
        "    print(f\"Predicted EP answer ==> {output}\")\n",
        "    print(f\"Similarity EP ==> {similarity}\")\n",
        "    if output != ignored_answer:\n",
        "      predicted_answers[output] = similarity\n",
        "\n",
        "    # for EPq in EPS2:\n",
        "    sec_input = output + EPS2[0]\n",
        "    if llm_mode == 'VICUNA':\n",
        "      sec_input = tokenizer_l(sec_input, return_tensors='pt')\n",
        "      sec_input = sec_input[\"input_ids\"].to(\"cuda\")\n",
        "    output, similarity = getAnswer(sec_input, rel_doc,llm_mode)\n",
        "    print(f\"Follow up EP answer ==> {output}\")\n",
        "    print(f\"Follow up Similarity EP ==> {similarity}\")\n",
        "    if output != ignored_answer:\n",
        "      predicted_answers[output] = similarity\n",
        "\n",
        "  if len(predicted_answers.keys()) != 0:\n",
        "    print(\"////////// Predicted Answers : \", predicted_answers)\n",
        "    theKey = max(predicted_answers, key=predicted_answers.get)\n",
        "    return theKey, predicted_answers[theKey]\n",
        "  else:\n",
        "    return \"\", 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "0WsSDA2TtmoO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHA92-SWO_NF"
      },
      "outputs": [],
      "source": [
        "def getBERTScore(predictions, ground_truth):\n",
        "  bertscore = evaluate.load(\"bertscore\")\n",
        "  bert_score = bertscore.compute(predictions=predictions, references=ground_truth, lang=\"en\")\n",
        "  print(f\"BERTScore: {bert_score}\")\n",
        "  return bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdKZEjNVVU0Y"
      },
      "outputs": [],
      "source": [
        "def getRelevantDoc(model, ind_mode, arr,  query_string, wb, map_wb , xq, k, s_mode, collection=None):\n",
        "  if ind_mode == 'LSH':\n",
        "    # LSH indexing and get top k relevant documents\n",
        "    print(f\"\\n================== {s_mode.upper()} LSH SEARCH ====================\\n\")\n",
        "    I, rel_docs = model.lsh_index_search(wb, xq, map_wb, k)\n",
        "  elif ind_mode == 'BM25':\n",
        "    # BM25 indexing and get top k relevant documents\n",
        "    print(f\"\\n================== {s_mode.upper()} BM25 SEARCH ====================\\n\")\n",
        "    I, rel_docs = model.bm25_search(arr, query_string, map_wb, k)\n",
        "  elif ind_mode == 'CHROMA':\n",
        "    # CHROMADB indexing and get top k relevant documents\n",
        "    print(f\"\\n================== {s_mode.upper()} CHROMADB SEARCH ====================\\n\")\n",
        "    I , rel_docs = model.chromadb_search(collection, xq.tolist(), map_wb, k)\n",
        "    print(rel_docs)\n",
        "    print(type(rel_docs))\n",
        "\n",
        "  links = \"\"\n",
        "  if s_mode == 'course':\n",
        "    for d in rel_docs:\n",
        "      delimiter = \"Course Description\"\n",
        "      split_parts = d.split(delimiter, 1)\n",
        "      links += f\"\\n {split_parts[0].split('Course link: ')[1]}\"\n",
        "  if s_mode == 'job':\n",
        "    for d in rel_docs:\n",
        "      split1 = d.split('Job link: ', 1)\n",
        "      links += f\"\\n {split1[1].split('Company name: ', 1)[0]}\"\n",
        "\n",
        "  doc_sim = model.cos_sim(I,xq)\n",
        "\n",
        "  print(f\" {ind_mode} search similarity : {doc_sim}\")\n",
        "  print(f\" {ind_mode} search relevant docs :{rel_docs}\")\n",
        "\n",
        "  print(f\"\\n================== END OF {s_mode.upper()} SEARCH ====================\\n\")\n",
        "\n",
        "  return rel_docs, links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqZgaeIzbmeh"
      },
      "outputs": [],
      "source": [
        "def downsize_context(query,rel_docs,no_docs_retrieved,context_size,model_l):\n",
        "  rel_docs = sorted(rel_docs, key=lambda x: len(x.split())) # smallest to largest\n",
        "  allocation = context_size//no_docs_retrieved\n",
        "  new_rel_doc = rel_docs[0]\n",
        "  temp_list = new_rel_doc.split()\n",
        "  if len(temp_list) > allocation:\n",
        "    # new_rel_doc = model_l(f\"Please summarise the context to strictly {allocation} words based on this question: {query}\\n Context:{rel_docs[0]}\")\n",
        "    temp_list = new_rel_doc.split()\n",
        "    if len(temp_list) > allocation:\n",
        "        temp_list = temp_list[:allocation]\n",
        "        new_rel_doc = ' '.join(temp_list)\n",
        "  if no_docs_retrieved == 1:\n",
        "    return \"Document: \" + new_rel_doc\n",
        "  else:\n",
        "    return \"Document: \" + new_rel_doc + downsize_context(query,rel_docs[1:],no_docs_retrieved-1,context_size-len(temp_list),model_l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XML4rQz0NWsq"
      },
      "outputs": [],
      "source": [
        "def dataset_setup(shuffle=True,seed=42):\n",
        "  jobs = pd.read_csv(\"drive/MyDrive/IR_Project/cleaned_data/jobs_cleaned.csv\")\n",
        "  courses = pd.read_csv(\"drive/MyDrive/IR_Project/sch_data/all.csv\")\n",
        "\n",
        "  jobs_arr = [Job(jobs.iloc[i]).job_text for i in range(jobs.shape[0])]\n",
        "  jobs_arr = [remove_stopwords(jobs_arr[i]) for i in range(len(jobs_arr))]\n",
        "  courses_arr = [Course(courses.iloc[i]).course_text for i in range(courses.shape[0])]\n",
        "  courses_arr = [remove_stopwords(courses_arr[i]) for i in range(len(courses_arr))]\n",
        "  random.seed(seed)\n",
        "  random.shuffle(jobs_arr)\n",
        "  random.shuffle(courses_arr)\n",
        "  return courses_arr, jobs_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpVwN4UGU61O"
      },
      "outputs": [],
      "source": [
        "def slice_dataset(courses_arr, jobs_arr, slicing=None):\n",
        "  jobs_arr = jobs_arr[:slicing]\n",
        "  courses_arr = courses_arr[:slicing]\n",
        "  return courses_arr, jobs_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtd0juchgG3E"
      },
      "source": [
        "# LLM Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS93Dun3McTm"
      },
      "outputs": [],
      "source": [
        "# Set your shit here\n",
        "llm_modes = ['VICUNA', 'OPENHERMES']\n",
        "enc_modes = ['BERT','MINILM','D2V']\n",
        "search_modes = ['LSH', 'BM25', 'CHROMA']\n",
        "\n",
        "llm_mode = llm_modes[0]\n",
        "encoding_mode = enc_modes[0]\n",
        "s_mode = search_modes[2]\n",
        "\n",
        "courses_arrs, jobs_arrs = dataset_setup(shuffle=True,seed=42) # Took 9 mins for me to stopword removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3b5pcqPafTP"
      },
      "outputs": [],
      "source": [
        "# Data setup\n",
        "courses_arr, jobs_arr = slice_dataset(courses_arrs, jobs_arrs, slicing=3000)\n",
        "####################################\n",
        "# LLM setup\n",
        "if llm_mode == 'VICUNA':\n",
        "  model_l = load_vicuna()\n",
        "elif llm_mode == 'OPENHERMES':\n",
        "  model_l = load_openhermes()\n",
        "tokenizer_l = LlamaTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
        "####################################\n",
        "# IR model setup\n",
        "irModel = IRModel()\n",
        "model = irModel.init_model(encoding_mode)\n",
        "course_wb, course_wb_vector = irModel.doc_to_vector(model,courses_arr)\n",
        "job_wb, job_wb_vector = irModel.doc_to_vector(model,jobs_arr)\n",
        "\n",
        "if s_mode == 'CHROMA':\n",
        "  jobCol = irModel.setup_chromadb(jobs_arr,job_wb.tolist(),\"job\")\n",
        "  courseCol = irModel.setup_chromadb(courses_arr,course_wb.tolist(),\"courses\")\n",
        "else:\n",
        "  jobCol = None\n",
        "  courseCol = None\n",
        "####################################\n",
        "heuristical = Heuristic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObOm-XQYRT91"
      },
      "outputs": [],
      "source": [
        "sample_qna = pd.read_csv(\"drive/MyDrive/IR_Project/Sample qna.csv\")\n",
        "questions = list(sample_qna[\"Question\"].values)\n",
        "good_ans = list(sample_qna[\"Good\"].values)\n",
        "\n",
        "index = 0\n",
        "questions = [questions[index]]\n",
        "good_answer = [good_ans[index]]\n",
        "questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hhzp5Hu69zJy"
      },
      "outputs": [],
      "source": [
        "#hyperparameters\n",
        "total_docs_retrieved = 7\n",
        "threshold = 0.8\n",
        "heuristic_weight = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRxc9NQSqXz8"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "answers = []\n",
        "print('\\n ///////////////////////////// START OF PREDICTION //////////////////////////////////////// \\n')\n",
        "\n",
        "for question in questions:\n",
        "  print(\"Questions:\", question)\n",
        "  tokenised_query = tokenizer_l(question, return_tensors='pt')\n",
        "  probability_of_token = heuristical.process(tokenised_query,heuristic_weight)\n",
        "  j_k =  math.floor(total_docs_retrieved*probability_of_token)\n",
        "  c_k =  total_docs_retrieved-j_k\n",
        "\n",
        "  query_vector = irModel.word_to_vector(model, question)\n",
        "\n",
        "  rel_doc_j, links_j = getRelevantDoc(irModel, s_mode, jobs_arr, question, job_wb, job_wb_vector, query_vector, j_k, \"job\",collection=jobCol)\n",
        "  rel_doc_c , links_c = getRelevantDoc(irModel, s_mode, courses_arr,  question, course_wb, course_wb_vector, query_vector, c_k, 'course',collection=courseCol)\n",
        "  rel_doc = rel_doc_j + rel_doc_c\n",
        "  if llm_mode == \"OPENHERMES\":\n",
        "    rel_doc = downsize_context(question,rel_doc,total_docs_retrieved,1536-len(question.split()),model_l) # 0.75 typical token to word ratio\n",
        "    context = \"\\n Context: \" + rel_doc\n",
        "  else:\n",
        "    context = '\\n Context: '\n",
        "    for theD in rel_doc:\n",
        "      context += theD\n",
        "  links = links_j + links_c\n",
        "\n",
        "  rel_doc_vector = irModel.word_to_vector(model,rel_doc)\n",
        "  prediction, similarity = getPredictionLOOOP(\"\", question, rel_doc_vector, 1, llm_mode)\n",
        "  # print(prediction)\n",
        "  # pred_vector = irModel.word_to_vector(model,prediction)\n",
        "\n",
        "  # similarity = irModel.cos_sim(pred_vector.reshape(1,-1), rel_doc_vector)\n",
        "  print(\"\\n==== Before Context Prediction : \", prediction ,\"====\\n\")\n",
        "  print(\"\\n==== Before Context Similarity : \", similarity, \"====\\n\")\n",
        "\n",
        "  bert_pred = [prediction]\n",
        "  current_acc = getBERTScore(bert_pred, good_answer)\n",
        "  print(\"Before precision: \", {np.mean(current_acc['precision'])})\n",
        "  print(\"Before recall: \", {np.mean(current_acc['recall'])})\n",
        "  print(\"Before f1: \", {np.mean(current_acc['f1'])})\n",
        "\n",
        "  if(similarity > threshold):\n",
        "      predictions.append([prediction, similarity])\n",
        "      print(f\"\\n==== Prediction for {question} is {prediction}\", \"====\\n\")\n",
        "  else:\n",
        "    print('\\n ///////////////////////////// STARTING SELF CONSISTENCY WITH CONTEXT //////////////////////////////////////// \\n')\n",
        "    # print('\\n ================================ CONTEXT ============================== \\n',context)\n",
        "    # print(\"\\n================================= END OF CONTEXT =================================================\\n\")\n",
        "\n",
        "    prediction_with_context, similarity_with_context = getPredictionLOOOP(context, question,rel_doc_vector, 5, llm_mode)\n",
        "    # prediction_with_context, similarity_with_context = getPredictionEP(context,question, rel_doc_vector, 2, llm_mode)\n",
        "    # vectors_with_context = irModel.word_to_vector(model,prediction_with_context)\n",
        "    # similarity_with_context = irModel.cos_sim(vectors_with_context.reshape(1,-1), rel_doc_vector)\n",
        "    print(\"\\n==== After Context Prediction : \", prediction_with_context,\"====\\n\")\n",
        "    print(\"\\n==== After Context Similarity : \", similarity_with_context, \"====\\n\")\n",
        "\n",
        "    if(similarity_with_context > similarity):\n",
        "      prediction = prediction_with_context + links\n",
        "      similarity = similarity_with_context\n",
        "    print(f\"\\n==== Prediction for {question} is {prediction}\", \"====\\n\")\n",
        "    predictions.append([prediction, similarity])\n",
        "    bert_pred = [prediction]\n",
        "    current_acc = getBERTScore(bert_pred, good_answer)\n",
        "    print(\"After precision: \", {np.mean(current_acc['precision'])})\n",
        "    print(\"After recall: \", {np.mean(current_acc['recall'])})\n",
        "    print(\"After f1: \", {np.mean(current_acc['f1'])})\n",
        "\n",
        "\n",
        "print('\\n ///////////////////////////// END OF PREDICTION //////////////////////////////////////// \\n')\n",
        "\n",
        "print(\"FINAL Prediction \" , predictions)\n",
        "bert_pred = [predictions[i][0] for i in range(len(predictions))]\n",
        "current_acc = getBERTScore(bert_pred, good_answer)\n",
        "print(f\"Overall mean precision: {np.mean(current_acc['precision'])}\")\n",
        "print(f\"Overall mean recall: {np.mean(current_acc['recall'])}\")\n",
        "print(f\"Overall mean f1: {np.mean(current_acc['f1'])}\")\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd1GfDjMaro9"
      },
      "outputs": [],
      "source": [
        "bert_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YslRRfJq4swh"
      },
      "outputs": [],
      "source": [
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGZEBqQVfVQd"
      },
      "outputs": [],
      "source": [
        "bert_pred = [predictions[i][0] for i in range(len(predictions))]\n",
        "current_acc = getBERTScore(bert_pred, good_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JubzdErMlVy"
      },
      "outputs": [],
      "source": [
        "print(len(predictions))\n",
        "for i in range(len(predictions)):\n",
        "  print(f'=== Answer for Question {i} ===')\n",
        "  print(f\"Prediction: {predictions[i][0]}\")\n",
        "  print(f\"Similarity: {predictions[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt1DBqVJuKrP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}